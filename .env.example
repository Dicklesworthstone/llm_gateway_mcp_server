# LLM Gateway MCP Server
# Environment Variables Configuration Example
# Copy this file to .env and fill in your values

# Server Configuration
SERVER_NAME=LLM Gateway
SERVER_PORT=8013
SERVER_HOST=0.0.0.0
SERVER_WORKERS=4
SERVER_DEBUG=false

# Logging Configuration
LOG_LEVEL=INFO                        
LOG_FILE=logs/llm_gateway.log         
USE_RICH_LOGGING=true                 

# Cache Configuration
CACHE_ENABLED=true                    
CACHE_TTL=86400                       
CACHE_DIR=.cache                      
CACHE_MAX_ENTRIES=10000               
CACHE_FUZZY_MATCH=true                

# Provider API Keys
OPENAI_API_KEY=sk-...                 
ANTHROPIC_API_KEY=sk-ant-...          
DEEPSEEK_API_KEY=sk-...               
GEMINI_API_KEY=...                    

# Provider Default Models
OPENAI_DEFAULT_MODEL=gpt-4o-mini      
ANTHROPIC_DEFAULT_MODEL=claude-3-5-haiku-latest 
DEEPSEEK_DEFAULT_MODEL=deepseek-chat 
GEMINI_DEFAULT_MODEL=gemini-2.5-pro-preview-03-25

# Provider Token Limits
OPENAI_MAX_TOKENS=8192              
ANTHROPIC_MAX_TOKENS=200000         
DEEPSEEK_MAX_TOKENS=8192            
GEMINI_MAX_TOKENS=8192              

# Vector Embedding Service
EMBEDDING_CACHE_DIR=.embeddings     
EMBEDDING_DEFAULT_MODEL=text-embedding-3-small 

# Advanced Configuration
REQUEST_TIMEOUT=60               
RATE_LIMIT_ENABLED=false         
MAX_CONCURRENT_REQUESTS=20       